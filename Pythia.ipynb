{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import the dataset from the Experimental Data Repository stored on a Google Drive\n",
        "\n",
        "We must import the dataset from the Experimental Data Repository that is stored on the Google Drive. To do this we first mount the drive, and then copy the contents back into our working space. We can then cd into it and get to work."
      ],
      "metadata": {
        "id": "sDpm056aVTbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Get the data From Drive\n",
        "%cp -r drive/MyDrive/maggie ./"
      ],
      "metadata": {
        "id": "zVTUheLaQhBn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd maggie"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juByPGsAR-fN",
        "outputId": "99a2d560-ab25-48af-dcd8-44d388d2c79b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/maggie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Append all the yearly data together\n",
        "Append all the yearly data together and store it all into one data frame. We use pandas to accomplish this and store the dates for later use"
      ],
      "metadata": {
        "id": "Y00EpILiuLIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data as a csv file\n",
        "df = pd.DataFrame()\n",
        "for i in range(2016, 2023 + 1):\n",
        "  yearframe = pd.read_csv(f'{i}.csv', header=None)\n",
        "  df = pd.concat([df, yearframe], ignore_index=True)\n",
        "dates = df.iloc[:, 0]\n",
        "df.drop(columns=df.columns[0], axis=1,  inplace=True)\n",
        "print(dates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0KnXfWne0E0",
        "outputId": "f613f13c-7e12-45c4-ff8e-c11ae173cc4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0          2016-01-01 00:00:00\n",
            "1          2016-01-01 00:01:00\n",
            "2          2016-01-01 00:02:00\n",
            "3          2016-01-01 00:03:00\n",
            "4          2016-01-01 00:04:00\n",
            "                  ...         \n",
            "3277435    2023-05-02 23:55:00\n",
            "3277436    2023-05-02 23:56:00\n",
            "3277437    2023-05-02 23:57:00\n",
            "3277438    2023-05-02 23:58:00\n",
            "3277439    2023-05-02 23:59:00\n",
            "Name: 0, Length: 3277440, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KP Ground Truth Dataset Collction\n",
        "We open a seperate dataset for the ground truth $k_p$ value. These scores will be compared with the model's output and used to make the model better. We test the valditity of these values below, testing if there are exactly $8$ for each day."
      ],
      "metadata": {
        "id": "zuxLGKWhbQIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the JSON KP Value Dataset\n",
        "import json\n",
        "with open(\"kp_values.json\") as f:\n",
        "  jsonstring = json.load(f)\n",
        "  kp_values = json.loads(jsonstring)\n",
        "\n",
        "for date, value in kp_values.items():\n",
        "  if(len(value) != 8):\n",
        "    print(\"Something went wrong!\")"
      ],
      "metadata": {
        "id": "NVgDbE81ZZiQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing\n",
        "We must first preform some computations on the data before we end up using it for our machine learning algorithms. First we convert to a numpy array for easy processing, then for every column in the observed spectra, we normalize the value to ensure that every value is between $[0, 1]$. Any values that are $0$ are considered invalid data, and so we input $-1$ into these to indicate a seperation between these values and the rest of the data."
      ],
      "metadata": {
        "id": "iMyadFySb3dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "combined_dataset = df.to_numpy()\n",
        "combined_dataset[np.isnan(combined_dataset).any(axis=1)] = np.zeros(53)\n",
        "normalized_data = combined_dataset[:, :3]\n",
        "for i in range(3, 53):\n",
        "  normalized_data = np.append(normalized_data, np.divide(combined_dataset[:, i:i+1], np.max(combined_dataset[:, i:i+1])), axis=1)\n",
        "normalized_data[normalized_data == 0] = -1\n"
      ],
      "metadata": {
        "id": "0ODyWYcLlUyH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sort ground truth values into labels set\n",
        "We sort the ground truth values and put them into a labels set to match the input data coming in"
      ],
      "metadata": {
        "id": "vrfBVx61dJFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "labels = []\n",
        "for day, values in sorted(kp_values.items(), key=lambda x: datetime.strptime(x[0], '%Y-%m-%d')):\n",
        "  for value in values:\n",
        "    labels.append(float(value))\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "TJf4YKbEhj1l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk together the data\n",
        "We know organize the data into chunks of size $180$. The reason we do this is to ensure that the smallest unit of time we can make a prediction over is $3$ hours.\n",
        "This makes sense since $k_p$ is only measured over a $3$ hour gap."
      ],
      "metadata": {
        "id": "MJgrsixhdyQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Split using numpy array functions\n",
        "chunks = np.array(np.split(normalized_data, normalized_data.shape[0] / 180))"
      ],
      "metadata": {
        "id": "vYP5dpuMRUY9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the data into a training and testing set\n",
        "We split the data into a training and testing set using a controlled parameter to determine the split. Note that we always use any validation/testing data to be future data because we want our model to generalize well into the future"
      ],
      "metadata": {
        "id": "_ed367Wcd7Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_training = int(len(chunks)*0.8)\n",
        "training = chunks[:num_training]\n",
        "testing = chunks[num_training:]"
      ],
      "metadata": {
        "id": "qfJk2AGxZVt-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reshape the training and testing data\n",
        "We reshape both datasets to match both X and Y as inputs to our model"
      ],
      "metadata": {
        "id": "K4MwsO69egZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_x = np.reshape(training, (len(training), 180, 53, 1))\n",
        "training_y = np.reshape(labels[:num_training], (len(training), 1))\n",
        "testing_x = np.reshape(testing, (len(testing), 180, 53, 1))\n",
        "testing_y = np.reshape(labels[num_training:len(chunks)], (len(testing), 1))"
      ],
      "metadata": {
        "id": "E-YmnrBvc0bW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN 2D Model\n",
        "Now with our data, we create our first model, a 2D CNN Model as a layer of 2D Convolutional Layers and Max Pooling Layers. We finally flatten the result and output a singular value for our estimated $k_p$ value. We then test and evaluate our model based on the training and testing data."
      ],
      "metadata": {
        "id": "WwAj2GpOesjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_uwPLPTWZHqk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (10, 10), activation='relu', input_shape=(180, 53, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (6, 6), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKctAYBVZN_T",
        "outputId": "94702de5-4980-4249-f0af-b639675475ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 171, 44, 32)       3232      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 85, 22, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 80, 17, 64)        73792     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 40, 8, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 38, 6, 64)         36928     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 14592)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                933952    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1047969 (4.00 MB)\n",
            "Trainable params: 1047969 (4.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras import backend\n",
        "backend.clear_session()\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "history = model.fit(training_x, training_y, epochs=3,\n",
        "                    validation_data=(testing_x, testing_y))\n",
        "\n",
        "model.save('2d_cnn.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHEl2ibKaESa",
        "outputId": "5b4e0e7f-7c34-4901-c0dc-07a891596e8e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "456/456 [==============================] - 462s 1s/step - loss: 1.5024 - mean_absolute_error: 0.9666 - val_loss: 1.4745 - val_mean_absolute_error: 0.9444\n",
            "Epoch 2/3\n",
            "456/456 [==============================] - 406s 891ms/step - loss: 1.4419 - mean_absolute_error: 0.9512 - val_loss: 1.3937 - val_mean_absolute_error: 0.9414\n",
            "Epoch 3/3\n",
            "456/456 [==============================] - 427s 937ms/step - loss: 1.4323 - mean_absolute_error: 0.9494 - val_loss: 1.3966 - val_mean_absolute_error: 0.9405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN 1D Model\n",
        "We now create our second model, as a layer of Convolutional 1D Layers together with MaxPooling1D Layers. We finally flatten the result and output a singular $k_p$ value. We then test and evaluate our model."
      ],
      "metadata": {
        "id": "GtSfp6KDygHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_x = np.reshape(training, (len(training), 180, 53))\n",
        "training_y = np.reshape(labels[:num_training], (len(training)))\n",
        "testing_x = np.reshape(testing, (len(testing), 180, 53))\n",
        "testing_y = np.reshape(labels[num_training:len(chunks)], (len(testing)))\n",
        "\n",
        "conv_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=32,\n",
        "                           kernel_size=(3,),\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(),\n",
        "    tf.keras.layers.Conv1D(filters=32,\n",
        "                        kernel_size=(3,),\n",
        "                        activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu', input_shape=(180, 53)),\n",
        "    tf.keras.layers.Dense(units=1),\n",
        "])\n",
        "\n",
        "conv_model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "\n",
        "history = conv_model.fit(training_x, training_y, epochs=10, validation_data=(testing_x, testing_y))\n",
        "\n",
        "conv_model.save(\"1d_cnn.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK74Fx7BmDze",
        "outputId": "ad210430-0412-4f48-8c74-fb125fb276be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "456/456 [==============================] - 9s 17ms/step - loss: 1.5177 - mean_absolute_error: 0.9812 - val_loss: 1.3846 - val_mean_absolute_error: 0.9367\n",
            "Epoch 2/10\n",
            "456/456 [==============================] - 8s 18ms/step - loss: 1.4856 - mean_absolute_error: 0.9726 - val_loss: 1.3598 - val_mean_absolute_error: 0.9359\n",
            "Epoch 3/10\n",
            "456/456 [==============================] - 6s 13ms/step - loss: 1.4801 - mean_absolute_error: 0.9699 - val_loss: 1.3616 - val_mean_absolute_error: 0.9387\n",
            "Epoch 4/10\n",
            "456/456 [==============================] - 8s 17ms/step - loss: 1.4783 - mean_absolute_error: 0.9703 - val_loss: 1.3631 - val_mean_absolute_error: 0.9369\n",
            "Epoch 5/10\n",
            "456/456 [==============================] - 6s 13ms/step - loss: 1.4776 - mean_absolute_error: 0.9709 - val_loss: 1.4041 - val_mean_absolute_error: 0.9352\n",
            "Epoch 6/10\n",
            "456/456 [==============================] - 8s 17ms/step - loss: 1.4767 - mean_absolute_error: 0.9721 - val_loss: 1.4650 - val_mean_absolute_error: 0.9661\n",
            "Epoch 7/10\n",
            "456/456 [==============================] - 6s 13ms/step - loss: 1.4761 - mean_absolute_error: 0.9719 - val_loss: 1.3633 - val_mean_absolute_error: 0.9363\n",
            "Epoch 8/10\n",
            "456/456 [==============================] - 8s 16ms/step - loss: 1.4750 - mean_absolute_error: 0.9708 - val_loss: 1.3628 - val_mean_absolute_error: 0.9391\n",
            "Epoch 9/10\n",
            "456/456 [==============================] - 6s 13ms/step - loss: 1.4729 - mean_absolute_error: 0.9706 - val_loss: 1.3923 - val_mean_absolute_error: 0.9395\n",
            "Epoch 10/10\n",
            "456/456 [==============================] - 9s 19ms/step - loss: 1.4724 - mean_absolute_error: 0.9703 - val_loss: 1.3581 - val_mean_absolute_error: 0.9380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model\n",
        "We present our LSTM Model. We unfortunatly did not save the specific training and testing data handling, but in this code snippet after that has been filled out this trains an LSTM model with 30 units on the training set and then evaluates it on the validation set"
      ],
      "metadata": {
        "id": "RykiiCd9zOlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training_x = ...\n",
        "# training_y = ...\n",
        "# testing_x = ...\n",
        "# testing_y = ...\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(30, return_sequences=False),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "history = lstm_model.fit(training_x, training_y, epochs=1, validation_data=(testing_x, testing_y))\n",
        "\n",
        "lstm_model.save('lstm.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQxvROZLO3Q3",
        "outputId": "124eb099-885e-4ee9-9482-efb787b5e643"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81936/81936 [==============================] - 1515s 18ms/step - loss: 1.2666 - mean_absolute_error: 0.8906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect predicted data for all years\n",
        "Load the desired model, and make a prediction for every year. Then append this data to a file to be used externally."
      ],
      "metadata": {
        "id": "8jHrBVqthHsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('2d_cnn.keras')\n",
        "prediction = model.predict(chunks)\n",
        "f = open(\"kp_data.txt\", \"w\")\n",
        "i = 0\n",
        "for date, _ in sorted(kp_values.items(), key=lambda x: datetime.strptime(x[0], '%Y-%m-%d')):\n",
        "  f.write(f\"{date},{labels[i]},{prediction[i][0]}\\n\")\n",
        "  i += 1\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAYbDbd1gxwh",
        "outputId": "8a8e6890-97c1-4fed-cc82-4f9c6866c82b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569/569 [==============================] - 127s 224ms/step\n"
          ]
        }
      ]
    }
  ]
}